"""
Ingestion_pipeline
DAG auto-generated by Astro Cloud IDE.
"""

from airflow.decorators import dag
from astro import sql as aql
import pandas as pd
import pendulum


"""
This pipeline demonstrates how to use the Astro Cloud IDE. It loads the Titanic dataset, filters out passengers under 18, and aggregates the data by survival and class.

The pipeline is composed of four cells:

`load`: Loads the Titanic dataset from Seaborn's GitHub repository.

`over_18`: Filters out passengers under 18.

`aggregate_sql`: Aggregates the data by survival and class, using SQL.

`aggregate_python`: Aggregates the data by survival and class, using Python.

Note that each cell returns a value that can be referenced in subsequent cells using the `{{cell_name}}` syntax in SQL and `cell_name` syntax in Python.Write your markdown hereâ€¦
"""

@aql.dataframe(task_id="load")
def load_func():
    import pandas as pd
    
    # use pandas to load the titanic dataset from github
    return pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv')

default_args={
    "owner": "Prashanth BS,Open in Cloud IDE",
}

@dag(
    default_args=default_args,
    schedule="0 0 * * *",
    start_date=pendulum.from_format("2024-05-01", "YYYY-MM-DD").in_tz("UTC"),
    catchup=False,
    owner_links={
        "Prashanth BS": "mailto:pbs@113industries.com",
        "Open in Cloud IDE": "https://cloud.astronomer.io/clvm6ngbd0awx01lp6bulu78s/cloud-ide/clvnd9cxx00uq01o1luruttuf/clvngp0vc00vp01qkr8eq4a8a",
    },
)
def Ingestion_pipeline():
    load = load_func()

dag_obj = Ingestion_pipeline()
